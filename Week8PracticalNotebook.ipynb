{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICAL 3 (WEEK 8): Samples, Normality and Regression predictions\n",
    "\n",
    "By the end of this practical you should be able to:\n",
    "* Select random samples from a dataset\n",
    "* Carry out linear regression, make a prediction and find its confidence interval\n",
    "* Test whether data are normally distributed\n",
    "\n",
    "Before we get started, remember we need to import the packages we need. We did this in the previous practical and we need to do it each time we start a new session with R.\n",
    "\n",
    "`library(tidyverse)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Random samples and estimators \n",
    "\n",
    "To investigate the properties of samples, we will work with the University of Florida graduate salaries dataset again. I have included it in the practical folder for this week already. Load this data into an object called `FloridaSalaries` in the usual way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean and standard deviation for the `salary` variable. Try and remember how to do this from previous practicals!\n",
    "\n",
    "HINT: Use `mean()` and `sd()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we also need to calculate the “Standard Error of the Mean”. This statistic is interpreted as if this was an 1100-member sample of a larger population, and helps us understand how confident we are that the sample mean reflects the actual population mean.\n",
    "\n",
    "Unfortunately, there is no direct way to do this in R so instead we will have to calculate it ourselves. Remember from the lecture how the standard error is the standard deviation divided by the square root of the sample size. Well, we can easily calculate this manually using the code below:\n",
    "\n",
    "`6967.98/(sqrt(1100))`\n",
    "\n",
    "Here, all we are doing is using R as a calculator and substituting the value of the standard deviation we calculated above (6967.98) and the sample size (i.e. the number of observations). In other words dividing the standard deviation (6967.98) by the square root of the sample size (1100) usingthe `sqrt()` function.\n",
    "\n",
    "This works fine but it is better practice to use the code below instead:\n",
    "\n",
    "`sd(FloridaSalaries$salary)/sqrt(length(FloridaSalaries$salary))`\n",
    "\n",
    "This is doing exactly the same thing but this time we are calculating the standard deviation and the sample size \"on-the-fly\" within the code using the `sd()` function that we have already. But we also use the `length()` function which tells us the length of the `FloridaSalaries` object (in other words the number of observations).  This second bit of code means if we were to ever use a slightly different dataset and run the code, it would do the calculation on the new updated values automatically (assuming variable and object names were the same)! \n",
    "\n",
    "Try both pieces of code in the cells below to verify that they give the same answer. The answers wont be **exactly** the same, can you think why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the lecture material, the standard error is interpreted as if this was an 1100-member sample of a larger population, and expresses the amount of uncertainty that the sample mean is the actual population mean. \n",
    "\n",
    "We are now going to demonstrate what happens to the standard error when the sample size is reduced. We will do this by taking random samples from the existing data. In R it is easy to create a random sample of a data set. Let's take the first sample using the code below. We will start with a sample of nine observations.\n",
    "\n",
    "`sampleSalary <- FloridaSalaries[sample(1:1100,9),]`\n",
    "\n",
    "You will rarely have need to do this in your own analyses so we dont need to spend time on understanding what the code is doing here (this is for ilustrative purposes only). All we really need to know here is that this command will randomly select 9 observations from the total of 1100 in the whole data (just note towards the end of the code where we specify the 9 sample size number). Once this command is submitted, a new data set is created, similar in nature to a subset, but randomly rather than based on a subset condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this new data set, find the mean and standard error of the mean using the same methods above.\n",
    "\n",
    "Is the mean similar to the mean of the overall data set? What about the standard error of the mean? Is this what you would expect, given the formula for the standard error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Repeat the steps above taking several more random samples of different sizes. For example to take a sample size of size 50 you would use:\n",
    "\n",
    "`sample50Salary <- FloridaSalaries[sample(1:1100,50),]`\n",
    "\n",
    "Note where the number 50 has been inserted towards the end of the code. \n",
    "\n",
    "Do this a few times (remembering to create new data object names for each sample!) with different sample sizes (maybe something like 20, 70, 150 but you can choose whatever you like as long as they significantly different to each other!). For each of the new samples do the following:\n",
    "\n",
    "1. Produce a histogram of the Salary variable\n",
    "2. Calculate the mean and standard error of the mean\n",
    "\n",
    "Are the histograms noticeably different (note that when looking at the widths of the histograms the x axis labels may not be consistent between the graphs!)? \n",
    "\n",
    "What about the mean and the standard error of the mean? Does this make sense given the formula for the standard error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution2": "hidden"
   },
   "source": [
    "Example code for calculating histograms, meand and standard deviation for a sample size 20:\n",
    "\n",
    "* Generate sample: `sample20Salary <- FloridaSalaries[sample(1:1100,20),]`\n",
    "* Generate a histogram (note how I am specifying the number of bins): `ggplot(sample20Salary, aes(salary)) + geom_histogram(bins=5)`\n",
    "* Calculate the mean and Sd: `mean(sample20Salary$salary)` and `sd(sample20Salary$salary)`\n",
    "\n",
    "The above code can be repeated for each sample size changing the code where relevant.\n",
    "\n",
    "**End of Solution**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Part 2: Linear regression revisited\n",
    "\n",
    "Last practical you learned how to find the coefficients for linear regression, along with the R-square value. As discussed in lecture, however, these coefficients have uncertainty associated with them. So now we will carry out linear regression and prediction properly.\n",
    "\n",
    "In the practical folder you will see a data files called **\"auto_ins.csv\"**. It contains two variables; **num_claims** and **payment**. The `num_claims` variable describes the number of auto insurance claims made by geographical region in Sweden  and the `payment` variable records payments made by insurance companies in each region (in thousands of Kronor). Data is sourced from the Swedish Committee on Analysis of Risk Premium in Motor Insurance. We are interested in predicting the amount of money paid out by insurers on the basis of the number of claims in each region. \n",
    "\n",
    "Read this data into you notebook in the usual way. Make sure the name you give the data object is `AutoInsurance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a linear regression model of the data with the \"payment\" variable as the y variable and \"num_claims\" as the explanatory (x) variable. Remember to save the model into an object with the name `insurance_model`.\n",
    "\n",
    "HINT: Remember from previous practicals you will need the `lm()` function and to view the model you need to use the `summary()` function.\n",
    "\n",
    "`insurance_model <- lm(formula = payment ~ num_claims, data = AutoInsurance)` \n",
    "\n",
    "`summary(insurance_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can see the coefficients of the linear regression: \n",
    "\n",
    "***Payment = m x (no_claims) + c***\n",
    "\n",
    "What we can see is that as the number of claims goes up so does the amount of payments. So each time the number of claims goes up by 1, the average payment increases by 3400 kronor. \n",
    "\n",
    "However, we want to look at the other values in the output this time around. Each coefficient has a Standard Error – this is very similar to the standard error of the mean, as it expresses the uncertainty as to the true value of the coefficients. To the right of this, we see p-values – although they are not labelled p-values, but “Pr(>|t|)”. The payment  coefficient p-value is very small – meaning it is **SIGNIFICANT** – i.e. the null hypothesis that m=0 is rejected. The p-value of the intercept is not as small, but it is still significant at the 0.05 level (since it is smaller than 0.05). \n",
    "\n",
    "Now, as you learned last week, create a scatterplot of the data including a line of best fit using the code below:\n",
    "\n",
    "`ggplot(AutoInsurance, aes(y=payment, x=num_claims)) + geom_point() + geom_smooth(method = \"lm\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the grey shaded region on the graph which shows the confidence interval for the slope of this line. In other words it shows the range of possible slopes that this line could take given that we have used a sample. Have a think how this relates to the information in the model summary output above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week we used the coefficients m and c to make a prediction of the average value of y given a certain of x, i.e. we found the predicted average tobacco retailer rate for a given level of deprivation. In reality, this prediction is inexact and contains uncertainty due to sampling error. R gives us a way to find both the predictions we want and also the confidence interval for those predictions, based on the regression model we produced above. The two pieces of code below do this for us:\n",
    "\n",
    "`dataval = data.frame(num_claims=50)`\n",
    "\n",
    "`predict(insurance_model, dataval, interval=\"confidence\")`\n",
    "\n",
    "Let's break these commands down. The first line creates a new variable (as opposed to a whole new dataset). The value is our test value for num_claims, i.e. 50 and is the value that we want to predict the value of payments from. The variable is called dataval – but you are free to use any name you like, as long as you use the same name in the second line. However, num_claims is the name of the independent variable in the regression (the explanatory variable).\n",
    "\n",
    "The second line does the prediction using the Linear Model (named \"insurance_model\") you created. R knows that we are using the results of the linear regression because insurance_model appears first in the code. It knows that we are inquiring about no_claims = 50 because of dataval, defined in the previous line. \n",
    "\n",
    "Run this code in the cells below. Note the first line will not create any output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see three numbers in the output. The first number (labelled \"fit\") is equal to  m x (50) + c, i.e. the linear prediction of the mean value of payments given a value of the number of claims being 50. The second and third numbers we have not seen before. These are the 95% lower and upper bounds of the prediction. Essentially, we should not trust the predicted value  completely. What we can trust is that the true prediction is between these two values with 95% confidence. In other words if we were to collect 100 random samples with the same information in each and repeat the same analysis in each sample, 95 of these samples would return estimates of the mean value (at num_claims = 50) that were between 177 and 205. When we produce statistical models on the basis of samples, this is how we account for the uncertainty that comes from using samples to say something about populations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Tests of normality \n",
    "\n",
    "Many statistical tests are based on the assumption that the data under consideration follow a normal distribution. As discussed in the lecture, there are several ways to test this assumption. Before we look at these tests though there is a problem we need to address.\n",
    "\n",
    "Up to this point all of the functions and commands we have needed have been included within either the core functions of R or as part of the \"tidyverse\" package that we have loaded each session. However, this week, we need an additional package, called \"nortest\", which contains key functions and commands we will need to do our tests of normality.\n",
    "\n",
    "Try the code below to see if we can load this package, in the same way we do the \"tidyverse\" package:\n",
    "\n",
    "`library(nortest)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an error appear that says something like **\"Error in library(nortest): there is no package called ‘nortest’\"**. \n",
    "\n",
    "This is because the \"nortest\" package (unlike the \"tidyverse\" package) hasn't actually been installed on our noteable system yet. So, we need to install it using the command below:\n",
    "\n",
    "`install.packages(\"nortest\")`\n",
    "\n",
    "Run this command below. When it completes it should output a red message which says \"done\" at the end. It may take a few seconds as the package installs so be patient! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try running the library command again (note it will not output anything when it is completed so dont worry!).\n",
    "\n",
    "`library(nortest)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should run without error and the package should now be ready to use. \n",
    "\n",
    "There are literally thousands of packages available than can be installed like this. If you ever come across an error like we saw above remember that we can use the `install.packages()` command to install missing packages.\n",
    "\n",
    "Ok, now let's do some of our tests of normality using the Tay river daily flow data set, and also the annual averages you produced last practical.\n",
    "\n",
    "Let's start with the TayAtBallathie.csv dataset (a new copy is in this week’s practical folder).\n",
    "\n",
    "<b>HINT</b>: Remember the following commands from previous weeks to help you:\n",
    "\n",
    "* `list.files()` - Lists all files in the current directory\n",
    "* `tay <- read_csv(\"TayAtBallathie.csv\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "## Exercise 2 \n",
    "\n",
    "Remind yourself of the distribution of the flowrate variable by writing code in the cells below to calculate the mean and standard deviation for the Flowrate variable. \n",
    "\n",
    "Next, produce a histogram of the Flowrate. \n",
    "\n",
    "From what we have learned about Normal Distributions, do you think daily flow is normally distributed based on your analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution2": "hidden"
   },
   "source": [
    "For the mean:\n",
    "\n",
    "`mean(tay$Flowrate)`\n",
    "\n",
    "For the standard Deviation:\n",
    "\n",
    "`sd(tay$Flowrate)`\n",
    "\n",
    "To produce a histogram:\n",
    "\n",
    "`ggplot(tay, aes(x=Flowrate)) + geom_histogram()`\n",
    "\n",
    "**End of solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now you will produce a Quantile-Quantile (Q-Q) plot of Flowrate. Use the code below to do this which will produce a plot and store it in a graph object called `flow_qq`. Remember because we are creating a new object we need to also to call the graph object to see it!\n",
    "\n",
    "`flow_qq <- ggplot(tay, aes(sample = Flowrate)) + geom_qq() + geom_qq_line(colour = \"blue\")`\n",
    "\n",
    "The code should be familiar to you, it is basically the same basic format as we have seen before, where we specify the data to be plotted, then the variables using `aes()` before the actual plots we wish to use i.e. with `+ geom...`. However, because we are producing a Q-Q plot the specification in the `aes()` brackets is a litte different because we have to use `sample = Flowrate`. `geom_qq` is the actual Q-Q plot and `geom_qq_line` is the expected line (more on this below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a Q-Q plot – with a blue line indicating the expected frequency within each quantile if the distribution was normal. Are the points (black dots) anywhere near this line? what does this suggest about whether our data are normally distributed?\n",
    "\n",
    "Let's now conduct formal tests to determine if this data really isnt normally distributed using a Kolmogorov-Smirnov (K-S) test. To do this we use the `lillie.test()` function from within the new \"nortest\" package we installed earlier. Note the name of the test `lillie` reflects the particular type of K-S test that we need. The code is really straightforward as you can see:\n",
    "\n",
    "`lillie.test(tay$Flowrate)`\n",
    "\n",
    "All we need is the function itself and the variable we want to test in the brackets (using the `$` convention to select the variable we want from the `tay` data object). \n",
    "\n",
    "Run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tells you the test which was just performed, the data and variable on which it was performed and gives a p-value. \n",
    "\n",
    "Is the p-value small? If it is less than 0.05, there is a good chance the distribution is non-normal. In formal terms we reject the null hypothesis if the p-value is less than 0.05. Remember, the K-S test is testing the alternative hypothesis that the data is **non-normally** distributed, so a low p-number means that we accept this hypothesis (and therefore reject the null hypothesis) and conclude that the data is **non-normal**. This can sometimes be confusing so keep this in mind!\n",
    "\n",
    "So, looking at the output above, what do we conclude? Our p-value is 2.2e-16, which is .00000000000000022. Quite a lot smaller than 0.05! \n",
    "\n",
    "Is our conclusion consistent with the Q-Q plot and the histogram?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week you created a data set of annual average flow in .csv format. I have added this to this weeks practical materials (filename is \"TayAvg.csv\"). Load this data into a data object called `TayAverage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "### Exercise 3\n",
    "\n",
    "Carry out the steps above for the `Flowrate` variable in the new data you have just loaded in (the aggregated tay flow data). So:\n",
    "\n",
    "1. Create a histogram\n",
    "2. Create a Q-Q plot\n",
    "3. Do a formal test to determine if it is normally distributed\n",
    "\n",
    "Do these sets of results make sense i.e. does what you see in the histogram and Q-Q plot correspond with the K-S test? Is the p-value for the K-S test of normality larger than for the full Tay dataset? Can we say this data is normally distributed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution2": "hidden"
   },
   "source": [
    "To produce the histogram use the following: (note how I changed the width of the histogram bins)... \n",
    "\n",
    "`ggplot(TayAverage, aes(Flowrate)) + geom_histogram(binwidth= 15)`\n",
    "\n",
    "Q-Q plot with added line:\n",
    "\n",
    "`ggplot(TayAverage, aes(sample = Flowrate)) + geom_qq() + geom_qq_line()`\n",
    "\n",
    "Kolmogorov-Smirnov test:\n",
    "\n",
    "`lillie.test(TayAverage$Flowrate)`\n",
    "\n",
    "Hopefully you saw that the K-S test p-value for the annual average data was much smaller than for the full Tay data. The p-value is much larger than 0.05 meaning we **cannot** reject the null hypothesis. We therefore conclude that the data is normally distributed.\n",
    "\n",
    "**End of solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As discussed in the lecture, there is another test for normality that is commonly used, known as the Shapiro-Wilk (S-W) test.  The command to produce a S-W test is almost identical to the K-S lillies test:\n",
    "\n",
    "`shapiro.test(TayAverage$Flowrate)`\n",
    "\n",
    "Run the test and compare the results to the K-S test that you ran above. Are the results similar to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you can see the similarity between the different tests and that again we can conclude the data are normally distributed based on evidence from both tests.\n",
    "\n",
    "There is one issue with the S-W test in that it is only really suitable for datasets with 5000 rows of data or less. It is more sensitive than the K-S test so with larger samples it is more likely to incorrectly show that a variable is not normally distributed. This means it isnt suitable for the main tay river data but it can be used on the aggregated tay data that you have created.\n",
    "\n",
    "To illustrate this, try running a S-W test on the full tay data using the below:\n",
    "\n",
    "`shapiro.test(tay$Flowrate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an error that says something like `stop(\"sample size must be between 3 and 5000\")`. This is helpful because R automatically prevents you running the test on inappropriate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution_first": true
   },
   "source": [
    "## Exercise 4\n",
    "\n",
    "You should still have the following data objects loaded in R:\n",
    "\n",
    "* `FloridaSalaries`\n",
    "* `AutoInsurance`\n",
    "\n",
    "Conduct tests of normality for the `salary` variable in `FloridaSalaries` and the `num_claims` and `payment` variables in the `AutoInsurance` data. You may want to produce Q-Q plots and histograms as well. Remember to keep in mind whether the S-W test is appropriate for a given variable.\n",
    "\n",
    "Which of these variables, if any, are normally distributed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution": "shown",
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution": "shown",
    "solution2": "hidden"
   },
   "source": [
    "For the `salary` variable in  `FloridaSalaries`:\n",
    "\n",
    "* Histogram: `ggplot(FloridaSalaries, aes(x=salary)) + geom_histogram()`\n",
    "* Q-Q plot: `ggplot(FloridaSalaries, aes(sample = salary)) + geom_qq() + geom_qq_line()`\n",
    "* K-S test: `lillie.test(FloridaSalaries$salary)`\n",
    "* S-W test: `shapiro.test(FloridaSalaries$salary)`\n",
    "\n",
    "For the `payment` variable in `AutoInsurance`:\n",
    "\n",
    "* Histogram: `ggplot(AutoInsurance, aes(x=payment)) + geom_histogram(bins=15)`\n",
    "* Q-Q plot: `ggplot(AutoInsurance, aes(sample = payment)) + geom_qq() + geom_qq_line()`\n",
    "* K-S test: `lillie.test(AutoInsurance$payment)`\n",
    "* S-W test: `shapiro.test(AutoInsurance$payment)`\n",
    "\n",
    "For the `num_claims` variable in `AutoInsurance`:\n",
    "\n",
    "* Histogram: `ggplot(AutoInsurance, aes(x=num_claims)) + geom_histogram(bins=15)`\n",
    "* Q-Q plot: `ggplot(AutoInsurance, aes(sample = num_claims)) + geom_qq() + geom_qq_line()`\n",
    "* K-S test: `lillie.test(AutoInsurance$num_claims)`\n",
    "* S-W test: `shapiro.test(AutoInsurance$num_claims)`\n",
    "\n",
    "Form this analysis it looks like none of the variables are normally distributed. The `payment` variable from the `AutoInsurance` data is the closest to being normally distributed but the p-value is still less than 0.05 which means we cannot formally conclude that the variable is normally distributed.\n",
    "\n",
    "**End of solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## End of Practical!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R with Stan",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
